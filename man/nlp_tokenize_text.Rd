% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nlp_tokenize_text.R
\name{nlp_tokenize_text}
\alias{nlp_tokenize_text}
\title{Tokenize text into a clean token stream}
\usage{
nlp_tokenize_text(
  corpus,
  by = c("doc_id", "paragraph_id", "sentence_id"),
  include_spans = TRUE,
  method = "word"
)
}
\arguments{
\item{corpus}{Data frame or data.table with a \code{text} column and the identifier columns specified in \code{by}.}

\item{by}{Character vector of identifier columns that define the text unit (e.g. \code{doc_id} or \code{c("url", "node_id")}). Default \code{c("doc_id", "paragraph_id", "sentence_id")}. The last column is the finest granularity.}

\item{include_spans}{Logical. Include start/end character spans for each token (default \code{TRUE}).}

\item{method}{Character. \code{"word"} or \code{"biber"}.}
}
\value{
Named list of tokens; or list of \code{tokens} and \code{spans} if \code{include_spans = TRUE}.
}
\description{
Normalize text into a clean token stream. Tokenizes corpus text, preserving
structure (capitalization, punctuation). The last column in \code{by} determines
the tokenization unit.
}
\examples{
corpus <- data.frame(doc_id = c('1', '1', '2'),
                    sentence_id = c('1', '2', '1'),
                    text = c("Hello world.",
                             "This is an example.",
                             "This is a party!"))
tokens <- nlp_tokenize_text(corpus, by = c('doc_id', 'sentence_id'))
}
