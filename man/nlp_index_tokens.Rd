% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nlp_index_tokens.R
\name{nlp_index_tokens}
\alias{nlp_index_tokens}
\title{Build a BM25 token index from tokenized documents}
\usage{
nlp_index_tokens(tokens, k1 = 1.2, b = 0.75, stem = FALSE)
}
\arguments{
\item{tokens}{A named list of character vectors. Each element is one document
(or text unit); names are document IDs. Typically the output of tokenizing
then splitting by document (e.g. from \code{\link{nlp_tokenize_text}} and
\code{\link{nlp_cast_tokens}}).}

\item{k1}{Numeric. BM25 term frequency saturation parameter (default 1.2).}

\item{b}{Numeric. BM25 length normalization parameter (default 0.75).}

\item{stem}{Logical. If \code{TRUE}, apply Porter stemming to tokens before
indexing. Requires the suggested package \pkg{SnowballC}.}
}
\value{
A \code{data.table} with columns \code{id}, \code{token}, \code{bm25},
  ordered by \code{id}.
}
\description{
Converts a named list of token vectors into a data.table with BM25 weights
per (id, token). English stopwords are removed and only alphabetic tokens
are kept. Used with \code{\link{search_index}} for ranked retrieval.
}
\examples{
tokens <- list(
  doc1 = c("the", "quick", "brown", "fox", "jumps"),
  doc2 = c("a", "quick", "dog", "runs", "fast")
)
idx <- nlp_index_tokens(tokens)
search_index(idx, query = "quick fox", n = 2)
}
